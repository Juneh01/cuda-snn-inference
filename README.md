# SpikeCUDA ğŸš€ï¼šcuda-snn-inference
a project from GPU Architecture & Programming course, University of Chinese Academy of Sciences

## High-Performance CUDA Implementation of Spiking Neural Network Inference

[![CUDA](https://img.shields.io/badge/CUDA-11.8-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Platform](https://img.shields.io/badge/Platform-Linux-orange.svg)]()

A highly optimized CUDA C++ implementation for Spiking Neural Network (SNN) inference on Fashion-MNIST dataset. This project achieves **~15ms inference time** for 10,000 images on NVIDIA V100 GPU, leveraging Tensor Cores, CUDA Graphs, and extensive kernel optimizations.

## ğŸ“‹ Table of Contents

- [Background](#-background)
- [Features](#-features)
- [Network Architecture](#-network-architecture)
- [Requirements](#-requirements)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Optimization Techniques](#-optimization-techniques)
- [Performance](#-performance)

## ğŸ¯ Background

Spiking Neural Networks (SNNs) are the third generation of neural networks that more closely mimic biological neural networks. Unlike traditional Artificial Neural Networks (ANNs), SNNs process information using discrete spikes over time, making them particularly suitable for neuromorphic computing and energy-efficient AI applications.

This project implements an optimized CUDA inference engine for a convolutional SNN trained on the Fashion-MNIST dataset. The network uses Integrate-and-Fire (IF) neurons and processes inputs over multiple timesteps (T=4).

### Key Characteristics of SNN

- **Temporal Dynamics**: Information is encoded in spike timing across T timesteps
- **Binary Activation**: Neurons output binary spikes (0 or 1) based on membrane potential
- **Membrane Potential**: Accumulates over time and resets after firing
- **Event-Driven**: Sparse, energy-efficient computation

## âœ¨ Features

- **High Performance**: ~15ms for 10,000 images on V100
- **Tensor Core Acceleration**: WMMA (Warp Matrix Multiply-Accumulate) for FC layers
- **CUDA Graph**: Reduced kernel launch overhead
- **PTX Intrinsics**: Low-level optimizations for memory access
- **Multi-Stream Pipeline**: Overlapped computation and data transfer
- **FP16 Optimization**: Half-precision weights for Tensor Core utilization

## ğŸ§  Network Architecture
### Side-by-side comparison between naive and optimized version
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         NAIVE VERSION            â”‚    â”‚       OPTIMIZED VERSION          â”‚
â”‚      (Sequential, 12 Kernels)    â”‚    â”‚   (Batched, 7 Fused Kernels)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                  â”‚    â”‚                                  â”‚
â”‚   Input (1 Ã— 28 Ã— 28)            â”‚    â”‚   Input (512 Ã— 28 Ã— 28)          â”‚
â”‚            â”‚                     â”‚    â”‚            â”‚                     â”‚
â”‚            â–¼                     â”‚    â”‚            â–¼                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚   Conv2D    â”‚ K1             â”‚    â”‚   â”‚ Conv2D + IF (Fused) â”‚ K1     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚   â”‚  + Shared Memory    â”‚        â”‚
â”‚          â–¼                       â”‚    â”‚   â”‚  + PTX Intrinsics   â”‚        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â”‚  IF Neuron  â”‚ K2             â”‚    â”‚             â”‚                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚             â–¼                    â”‚
â”‚          â–¼                       â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â”‚     MaxPool 2Ã—2     â”‚ K2     â”‚
â”‚   â”‚  MaxPool    â”‚ K3             â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚             â”‚                    â”‚
â”‚          â–¼                       â”‚    â”‚             â–¼                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚   Conv2D    â”‚ K4             â”‚    â”‚   â”‚ Conv2D + IF (Fused) â”‚ K3     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚   â”‚  + Ping-Pong Buffer â”‚        â”‚
â”‚          â–¼                       â”‚    â”‚   â”‚  + Software Pipelineâ”‚        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â”‚  IF Neuron  â”‚ K5             â”‚    â”‚             â”‚                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚             â–¼                    â”‚
â”‚          â–¼                       â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â”‚     MaxPool 2Ã—2     â”‚ K4     â”‚
â”‚   â”‚  MaxPool    â”‚ K6             â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚             â”‚                    â”‚
â”‚          â–¼                       â”‚    â”‚             â–¼                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚     FC1     â”‚ K7             â”‚    â”‚   â”‚  FC1 + IF (WMMA)    â”‚ K5     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚   â”‚  Tensor Core FP16   â”‚        â”‚
â”‚          â–¼                       â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚             â”‚                    â”‚
â”‚   â”‚  IF Neuron  â”‚ K8             â”‚    â”‚             â–¼                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚          â–¼                       â”‚    â”‚   â”‚  FC2 + IF (WMMA)    â”‚ K6     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â”‚  Tensor Core FP16   â”‚        â”‚
â”‚   â”‚     FC2     â”‚ K9             â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚             â”‚                    â”‚
â”‚          â–¼                       â”‚    â”‚             â–¼                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚  IF Neuron  â”‚ K10            â”‚    â”‚   â”‚ FC3 + Accumulate    â”‚ K7     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚   â”‚  Float4 Vectorized  â”‚        â”‚
â”‚          â–¼                       â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”‚             â”‚                    â”‚
â”‚   â”‚     FC3     â”‚ K11            â”‚    â”‚             â–¼                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚      Output (512 Ã— 10)           â”‚
â”‚          â–¼                       â”‚    â”‚                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   â”‚ Accumulate  â”‚ K12            â”‚    â”‚  Additional Optimizations:       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚  â€¢ CUDA Graph                    â”‚
â”‚          â–¼                       â”‚    â”‚  â€¢ Multi-Stream Pipeline         â”‚
â”‚      Output (10)                 â”‚    â”‚  â€¢ Pinned Memory                 â”‚
â”‚                                  â”‚    â”‚  â€¢ Async Transfers               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kernels: 12                     â”‚    â”‚  Kernels: 7                      â”‚
â”‚  Batch Size: 1                   â”‚    â”‚  Batch Size: 512                 â”‚
â”‚  Memory Access: Naive            â”‚    â”‚  Memory Access: Optimized        â”‚
â”‚  Compute: FP32 only              â”‚    â”‚  Compute: FP32 + FP16 Tensor     â”‚
â”‚  Timesteps: 8                    â”‚    â”‚  Timesteps: 4                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Timesteps**: T=4 (network processes each input over 4 time steps)

## ğŸ’» Requirements

### Hardware

- NVIDIA GPU with Compute Capability â‰¥ 7.0 (Volta or newer)
- Recommended: Tesla V100, RTX 2080 or newer

### Software

- CUDA Toolkit 11.8
- GCC/G++ with C++14 support
- Linux (Ubuntu 20.04+ recommended)

### For Training (Optional)

- Python 3.12
- PyTorch 2.6.0
- SpikingJelly


## ğŸ”§ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/Juneh01/cuda-snn-inference.git
cd cuda-snn-inference
```

### 2. Download Dataset

Download Fashion-MNIST dataset and place it in the `data` directory:

```bash
mkdir -p data/FashionMNIST/raw
cd data/FashionMNIST/raw
# Download the following files:
# - t10k-images-idx3-ubyte
# - t10k-labels-idx1-ubyte
```


### 3. Prepare Weights

Use the pre-trained weights provided in `weights/` directory, or train your own:

```bash
# Using pre-trained weights
cp weights/*.txt ./here/are/weights

# Or train your own (requires Python environment)
python train.py
```

### 4. Compile

On the course evaluation system (V100)
```bash
nvcc inference_optimized.cu -o inference_optimized_prog \
    -Xcompiler "-O3 -std=c++14" \
    -gencode arch=compute_70,code=sm_70 \
    -rdc=true
```

For different GPU architectures or local evaluation:

```bash
# For Ada Lovelace (my RTX 4070 Super)
nvcc inference_optimized.cu -o ./inference_optimized_prog \
    -Xcompiler "-O3 -std=c++17" \
    -gencode arch=compute_89,code=sm_89 \
    -rdc=true

# For multiple architectures
nvcc inference_optimized.cu -o inference_optimized_prog \
    -Xcompiler "-O3 -std=c++14" \
    -gencode arch=compute_70,code=sm_70 \
    -gencode arch=compute_75,code=sm_75 \
    -gencode arch=compute_80,code=sm_80 \
    -rdc=true
```

## ğŸš€ Usage

### Basic Usage

```bash
./inference_prog <path_to_weights_directory>
```

### Example

```bash
./inference_optimized_prog ./here/are/weights/
```

### Output Format

```
<inference_time>:<accuracy>
```

Example output:

```
0.0154:0.8989
```

- Inference time: 0.0154 seconds (15.4 ms)
- Accuracy: 89.89%



## ğŸ“ Project Structure

```
cuda-snn-inference/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ LICENSE                   # MIT License
â”œâ”€â”€ inference.cu              # Main CUDA inference implementation
â”œâ”€â”€ here/are/weights/                  # Pre-trained model weights
â”‚   â”œâ”€â”€ conv1.weight.txt
â”‚   â”œâ”€â”€ conv1.bias.txt
â”‚   â”œâ”€â”€ conv2.weight.txt
â”‚   â”œâ”€â”€ conv2.bias.txt
â”‚   â”œâ”€â”€ fc1.weight.txt
â”‚   â”œâ”€â”€ fc1.bias.txt
â”‚   â”œâ”€â”€ fc2.weight.txt
â”‚   â”œâ”€â”€ fc2.bias.txt
â”‚   â”œâ”€â”€ fc3.weight.txt
â”‚   â””â”€â”€ fc3.bias.txt
â”œâ”€â”€ data/                     # Dataset directory
â”‚   â””â”€â”€ FashionMNIST/
â”‚       â””â”€â”€ raw/
â”‚           â”œâ”€â”€ t10k-images-idx3-ubyte
â”‚           â””â”€â”€ t10k-labels-idx1-ubyte
â””â”€â”€ train.py                 # Training scripts (Python)
```

## âš¡ Optimization Techniques

### 1. Tensor Core (WMMA)

- FC1 and FC2 layers use Warp Matrix Multiply-Accumulate
- FP16 weights pre-converted for Tensor Core efficiency
- 16Ã—16Ã—16 tile size for optimal utilization

### 2. CUDA Graph

- Captures entire timestep loop (28 kernel launches)
- Eliminates CPU-GPU synchronization overhead
- ~1-2ms savings per batch

### 3. PTX Intrinsics

```cuda
// Fused Multiply-Add
asm("fma.rn.f32 %0, %1, %2, %3;" : "=f"(result) : "f"(a), "f"(b), "f"(c));

// Non-coherent cache load
asm("ld.global.nc.f32 %0, [%1];" : "=f"(result) : "l"(ptr));

// Vectorized load (float4)
asm("ld.global.nc.v4.f32 {%0, %1, %2, %3}, [%4];" ...);
```

### 4. Memory Optimization

- Shared memory for weights and intermediate results
- Pinned host memory for async transfers
- Coalesced global memory access patterns

### 5. Multi-Stream Pipeline

- 2 CUDA streams for overlapped execution
- Async memory transfers (H2D and D2H)
- Double buffering for continuous processing

### 6. Kernel Fusion

- Conv + IF neuron fused into single kernel
- Reduced global memory traffic
- Software pipelining with ping-pong buffers

## ğŸ“Š Performance

### Benchmark Results (V100-PCIE-32GB)

| Metric         | Value               |
| -------------- | ------------------- |
| Total Images   | 10,000              |
| Batch Size     | 512                 |
| Timesteps (T)  | 4                   |
| Inference Time | ~15.4 ms            |
| Throughput     | ~645,000 images/sec |
| Accuracy       | 89.89%              |

### Performance Breakdown

| Component           | Estimated Time |
| ------------------- | -------------- |
| Data Transfer (H2D) | ~2 ms          |
| Conv1 + IF          | ~3 ms          |
| Pool1               | ~0.5 ms        |
| Conv2 + IF          | ~2 ms          |
| Pool2               | ~0.3 ms        |
| FC1 (WMMA)          | ~2 ms          |
| FC2 (WMMA)          | ~1.5 ms        |
| FC3                 | ~1 ms          |
| Data Transfer (D2H) | ~0.5 ms        |
| Overhead            | ~2 ms          |

### Comparison

| Implementation           | Time (ms) | Speedup |
| ------------------------ | --------- | ------- |
| Naive CUDA               | ~4903     | 1Ã—      |
| Optimized (this project) | ~15.4     | ~318Ã—   |
| Theoretical Limit*       | ~4        | -       |

*Based on memory bandwidth analysis

## ğŸ”¬ Training (Optional)

### Setup Python Environment

```bash
conda create -n snn-cuda python=3.12
conda activate snn-cuda
pip install torch==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu118
pip install spikingjelly
```

### Train Model

```bash
python train.py
```

Training parameters can be modified in `train.py`:

- Epochs: 100
- Batch size: 128
- Learning rate: 1e-3
- Timesteps: 4



## ğŸ™ Acknowledgments

- Course: GPU Architecture and Programming (2025Fall) UCAS
- Framework: [SpikingJelly](https://github.com/fangwei123456/spikingjelly)
- Dataset: [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)

## ğŸ“§ Contact

For questions or suggestions, please open an issue or contact the maintainer.

---

**Made with â¤ï¸ and CUDA**